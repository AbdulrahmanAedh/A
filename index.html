<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>AbdulrahmanAedhWebsit</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>Welcome to my </strong> Portfolio Website</a>
									<ul class="icons">
										<li><a href="https://www.linkedin.com/in/abdulrahmanaedh" target="_blank" class="icon brands fa-linkedin"><span class="label">linkedin</span></a></li>
										<li><a href="https://github.com/AbdulrahmanAedh?tab=repositories" target="_blank" class="icon brands fa-github"><span class="label">github</span></a></li>
										<li><a href="https://twitter.com/vip555a1" target="_blank" class="icon brands fa-twitter"><span class="label">fa-twitter</span></a></li>
										<li><a href="images/AAEDH_Resume.pdf" target="_blank" class="icon fas fa-copy"></i><span class="label"> PDF</span></a></li>

									</ul>
								</header>
							<!-- Banner -->
								<section id="banner">
									<div class="content">
										<header>
											<h2>Hi, I'm Abdulrahman Aedh, Master of Data Science from the University of St. Thomas in Minnesota.</h2>
											<p>Candidate skilled in data science, ML, AI,deep learning, data engineer, data warehousing, Cloud services and data visualization. A creative problem solver, fast learner with 
												strong verbal and written communication skills.</p>	
									</div>
									<span class="image object">
										<img src="images/profile-pic (4).png",  alt="Avatar" />
									</span>
								</section>

							</header>
							<h2>Projects      </h2>
							<h3>1- Big Data Analysis   </h3>
							
							
							
								<form action="https://github.com/AbdulrahmanAedh/IDMb-review-Dataset-Analysis-/" method="get" target="_blank">
								   <button type="submit">View Code on Github</button>
								</form>
							 
		                <p>
							<p> The IMDb Review Dataset is 3 GB with information collected from Internet Movie Database (IMDb). This online database consists of reviews, cast and crew 
								information, trivia, biographies, plot summaries, and critic rates of films, TV shows, video games, and streaming content. This curated dataset centers 
								on text reviews of TV shows and movies and also includes the review_id (generated by IMDb), reviewer (public username of the reviewer), movie (title of 
								movie or TV show), rating (out of 10, or none), review_summary (text summary of review), review_date, spoiler-tag (1 if spoiler in review, 0 if not), 
								review_detail (more detailed review), and helpful (how many people found the review helpful). This large dataset has 5,571,499 total records, with 
								1,699,310 unique users contributing reviews and 1,186,611 reviews marked as “spoiler”.    
							
							</p>

								 <figure>
									<figcaption> <strong>    A quick look at imdb-review-dataset </strong> </figcaption>

						
							<img src="images/Screen Shot 2023-04-04 at 11.37.32 PM.png" alt="Description of the image", width="1200" height="150">
						       
						    

						
						  
							
							<h4> Tools</h4>
							<p> The large volume of data requires powerful resources to process, so the best tool is Apache Spark. I used the Databricks Community Edition platform 
								on the cloud, which gives a free virtual cluster to perform parallel processing and fast analytic queries against data of any size. </p>


							<h4> Data Collection</h4>
							<p> First, get Kaggle API Key and store it in the Databricks file store, then fetch the data via API key and store it in the DBFS. Install Kaggle 
								and change the file access on the API key. API fetches a large ZIP file unzipped to 6 different JSON files. After figuring out how to unzip this, 
								so it takes time to figure out how to load the JSON files and, append them all together, then convert them to a data frame (Pandas).   </p>

								<figure>
									<figcaption> <strong>Uploading imdb-review-dataset to DBFS</strong> </figcaption>

								 <img src="images/Screen Shot 2023-04-04 at 10.59.16 PM.png" alt="Description of the image", width="1200" height="150">
								 <img src="images/Screen Shot 2023-04-04 at 11.07.42 PM.png" alt="Description of the image", width="1200" height="150">

								 </figure>

								 
				
                            <h4>Data Preparation  </h4>

							<p>The dataset in its original state had been relatively cleaned and normalized before ever touching it—the column variables. Based on the analysis to do
							 and the real-world questions I wanted to be answered, it first started with removing columns that will not be used in data analysis, review_id and helpful. 
							 Next, explore the data by peering at the rating column—specifically, seeing which movies had a rating of NaN and NoneType; get rid of them before analysis so
							  it doesn’t throw off any of the statistics. Since this is the only column with NaNs and NoneTypes, remove all those rows, which reduced the dataset by nearly
							   15%, and delete any duplicate entries, further reducing data. Finally, I checked to see the datatypes for all the columns. Since almost all of them were strings, 
							   I changed two datatypes to perform data analysis calculations: rating to int and review_date to datetime, via pandas.  </p>
 
						    <h4>Data Analysis </h4>
							<p> I have done several analyses and statistics of dates, reviews, and movies. I focus on interesting insights with respect to review dates, so first focused on which
								 month were the most reviews have written, which gave December, with January as a close second. Then questions were asked: Which dates were the most reviews written? 
								 Which single date had the most reviews written? How many unique dates are there? So, the panda's function value.counts() to ascertain how many unique date counts there 
								 are in the review_date column and then establishes the length of that column to answer that question: there are 8,202 unique dates in the cleaned dataset. </p>
								 <img src="images/Screen Shot 2023-04-05 at 12.00.41 AM.png" alt="Description of the image", width="1200" height="70">
								 <figcaption> <strong>Count reviews each month</strong> </figcaption>
								 <img src="images/Screen Shot 2023-04-05 at 1.02.27 AM.png" alt="Description of the image", width="1200" height="300">

								 

								 

							<p> Next, I wanted to look at not only the single date with the most reviews but the top 10 dates with the most reviews. Since I already did unique date counts (which also sort
								 in descending order). Then, I grabbed the top 10 dates and their respective counts as well and noticed something interesting: 8 out of the 10 dates with the most amount of 
								 reviews happened to all have been during the pandemic (2020), and many of them (6 out of 10) of the dates happen to have been right after the Thanksgiving and Christmas holiday. 
								 The theory is that, while everyone was locked down and (presumably) staying home for the holidays, not traveling, people watched movies (not unusual for any holiday, even not 
								 during a pandemic) and not being able to do much else that day, decided to write a review on IMDb of the movies they just watched.  </p>
								 <img src="images/Screen Shot 2023-04-05 at 12.03.00 AM.png" alt="Description of the image", width="1200" height="200">
								 <figcaption> <strong>Count reviews per year</strong> </figcaption>
								 <img src="images/Screen Shot 2023-04-05 at 12.42.21 AM.png" alt="Description of the image", width="1200" height="400">

								 


							<p> The second analysis I focused on with interesting insights was the movies with the most reviews. Much like the review_date analysis previously mentioned, using pandas value.counts() 
								function to get the number of  (unique) reviews for each movie, automatically sorting it in descending order, then grabbing the movie at the first position (the movie with the most reviews),
								 which happens to be Avengers: Endgame (with 8,673 reviews). Afterward, we grab the top 10 movies and their respective review counts. No surprise that 4 out of the top 10 are movies from the 
								 Marvel Universe. Only 2 out of the top 10 were non-English movies. Since the Movie variable in this dataset also includes TV shows and their respective episodes, we also see that the very 
								 last episode of the Game of Thrones series is in the 4th position at 7,261 reviews, and given the amount of divisive opinions about how the show ended and the popularity of the show, we should 
								 not be surprised at all to see it on the list. </p>
								 <img src="images/Screen Shot 2023-04-05 at 12.19.37 AM.png" alt="Description of the image", width="1200" height="250">
								 <img src="images/Screen Shot 2023-04-05 at 12.39.11 AM.png" alt="Description of the image", width="1200" height="300">
								 

							<p> A fascinating analysis was a word count analysis of the review_detail variable, which is the text of user reviews on a single JSON sample file. 
								First, explore this sample data a little bit by printing the first two entries of text reviews, then using the MapReduce and flatmap function to split 
								the strings, lower the case, and filter out words that have a length of less than 5 letters (to avoid small, meaningless words). then sort all the 
								words by count and can see the top 10 most frequent words in the review text, which includes the word “movie” (most frequent word) and other words
								 like “great”, “story”, and “people”. 
							</p>
							<img src="images/Screen Shot 2023-04-05 at 12.34.49 AM.png" alt="Description of the image", width="1200" height="200">

						
						</p>
                    
						
						
							<h3> 2- Analysis of Retail Invoice Data Using Apache Spark</h3>
							<body>
								<form action="https://github.com/AbdulrahmanAedh/Invoice-Data-Analysis-" method="get" target="_blank">
								   <button type="submit">View Code on Github</button>
								</form>
							 </body>

							
							<p> Analysis of Retail dataset from Databricks Community Edition datastore. Reading dataset by SqlContext and converting it to resilient distributed 
								dataset (RDD), then using lambda (MapReduce) function to answer real-world business questions. That could help the business owners to get an insight 
								and evaluate business performance about customers' purchases by analyzing invoices. Dataset has 8 columns: "InvoiceNo" Invoice number, "StockCode" Stock number for each product, 
								"Description" Item Description, "Quantity" The number of items sold, "InvoiceDate" The date of the purchase, "UnitPrice" in dollar, "CustomerID" Customer number 
								for each purchase, "Country" In which country was the purchase made?. 
								</p>
								<figcaption> <strong>    A quick look at Retail Invoice dataset first five rows</strong> </figcaption>

								<img src="images/Screen Shot 2023-04-05 at 5.47.05 AM.png" alt="Description of the image", width="1200" height="150">
							<h4> 
								The first question to answer is:	Which customer in the dataset has spent the most on products?
							</h4>
                            <p>
								The answer is: The quantity multiplied by the unit price will give the total dollar amount spent per invoice line; the customer with ID = 18102 has spent the most 
								on products with the amount of 27834.61 dollars. Below is the result of Spark code:
								
								
							</p>

							<img src="images/Screen Shot 2023-04-05 at 6.14.16 AM.png" alt="Description of the image", width="1200" height="150"> 

							<h4> 
								The second question to answer is:	What is the product description for the best selling product in the dataset?
							</h4>

							<p>
								The answer is: Define "Best Selling" as the product with the highest quantity sold.
								
								Below is the result of Spark code: 
								
								
							</p>

							<img src="images/Screen Shot 2023-04-05 at 6.30.20 AM.png" alt="Description of the image", width="1200" height="150"> 

							<h4> 
								The third question to answer is:	How much has each country spent on products?
							</h4>

							<p>
								The answer is that the quantity multiplied by each country's unit price will give the total dollar 
								amount spent per invoice line. The United Kingdom has the highest spending on products with an amount of 965042.74 dollars, 
								and Switzerland was the least spending on products with an amount of 4909.54 dollars. 
								
								
								
							</p>
							<p>Below is the result of the Spark code with a pretty visualization:</p>

							<img src="images/Screen Shot 2023-04-05 at 6.39.15 AM.png" alt="Description of the image", width="1200" height="150"> 
							<img src="images/Screen Shot 2023-04-05 at 6.41.20 AM.png" alt="Description of the image", width="1235" height="400"> 


						    <h4> 
								The forth question to answer is:   What is the highest-grossing day in the dataset?
						    </h4>

						    <p>
								The answer is that the quantity is multiplied by the unit price to get the revenue per line. The highest grossing day was 
								January 11/2011.

								
								
						    </p>
						    <p>Below is the result of the Spark code with a pretty visualization:</p>

						      <img src="images/Screen Shot 2023-04-05 at 8.22.38 AM.png" alt="Description of the image", width="1235" height="150"> 
							  <img src="images/Screen Shot 2023-04-05 at 8.44.05 AM.png" alt="Description of the image", width="1235" height="120"> 
							  
							  <img src="images/Screen Shot 2023-04-05 at 8.35.43 AM.png" alt="Description of the image", width="1235" height="350"> 

							  

                       

                        
						   
						</p>
						
							<a href="index.html" class="logo"> <strong> Social Media </strong> Links</a>
							<ul class="icons">
									<li><a href="https://www.linkedin.com/in/abdulrahmanaedh/" class="icon brands fa-linkedin"><span class="label">linkedin</span></a></li>
									<li><a href="https://github.com/AbdulrahmanAedh?tab=repositories" class="icon brands fa-github"><span class="label">github</span></a></li>
									<li><a href="https://twitter.com/vip555a1" class="icon brands fa-twitter"><span class="label">fa-twitter</span></a></li>
									<li><a href="images/AAEDH_Resume.pdf" target="_blank" class="icon fas fa-copy"></i><span class="label"> PDF</span></a></li>



							</ul>
							
                        
						</div>
					</div>

			</div>

		
	</body>
</html>